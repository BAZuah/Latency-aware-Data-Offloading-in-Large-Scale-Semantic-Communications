# -*- coding: utf-8 -*-
"""QAOA_ multiuser.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nhrdhDQyToZj5kszJddRyvZ0YkYU6AYH
"""

# QAOA for latency-minimizing power selection (one-hot), PennyLane
# Discrete power levels, Shannon rate, semantic quality mask, XY ring mixer

import itertools
import numpy as np
import pennylane as qml
from pennylane import qaoa
import matplotlib.pyplot as plt

# ---------------------------
# Problem parameters
# ---------------------------
N_users_list = [1, 10, 50]

# Link and environment
B_total_system = 2.0e6
eta = 0.7
n0_dBmHz = -174
n0 = 10**((n0_dBmHz - 30) / 10)

# Channel gain
h = 1.0e-9

# Payload from fixed semantic encoder
d_bits = 50952 * 10
rho_k = 0.3
T_enc = 1.0

# Semantic fidelity mask
z_req = 5.0
Delta = 1.0e3

# Discrete power levels (Watts)
power_levels = np.array([0.005, 0.01, 0.02, 0.05, 0.08, 0.1])
L = len(power_levels)

# QAOA hyperparameters
depth_p = 2
steps = 100
stepsize_init = 0.035
stepsize_final = 0.002
seed = 7

# Device
dev = qml.device("default.qubit", wires=L, shots=None)

def build_problem_for_N_users(N_users):
    """Build cost tables and Hamiltonians for given number of users"""

    B_per_user = B_total_system / N_users
    a = h / (B_per_user * n0)

    def rate_bits_per_sec(p):
        return eta * B_per_user * np.log2(1.0 + a * p)

    def c_tx(p):

        if p <= 0: return np.inf
        return d_bits * (1.0 - rho_k) / rate_bits_per_sec(p)

    def c_total(p):
        """Calculate TOTAL latency"""
        if p <= 0: return np.inf
        return T_enc + c_tx(p)

    def quality_mask(p):
        """Semantic quality mask based on SNR threshold"""
        z = a * p
        return 0.0 if z >= z_req else Delta

    # Build cost tables
    c_tx_table = np.array([c_tx(p) for p in power_levels], dtype=float)
    c_total_table = np.array([c_total(p) for p in power_levels], dtype=float)
    m_table = np.array([quality_mask(p) for p in power_levels], dtype=float)
    classical_costs = c_total_table + m_table
    print("--- Problem Setup (Multiuser, MATCHED TO SCA) ---")
    print(f"Number of users: {N_users}")
    print(f"Total system bandwidth: {B_total_system:.2e} Hz")
    print(f"Bandwidth per user: {B_per_user:.2e} Hz (= B_total/N)")
    print(f"Data bits per user: {d_bits:.2e} bits")
    print(f"Encoding latency: {T_enc:.2f} s")
    print(f"SNR slope (a): {a:.2e}")
    print(f"Feasible (m=0) / Infeasible (m={Delta}) mask:\n{m_table}")
    print(f"Transmission latency table (c_tx):\n{c_tx_table.round(3)}")
    print(f"Total latency table (T_enc + c_tx):\n{c_total_table.round(3)}")
    print(f"Total cost (c_total + m) to be minimized:\n{classical_costs.round(3)}")
    print(f"Expected best index: {np.argmin(classical_costs)}\n" + "-"*20 + "\n")

    return c_total_table, m_table, classical_costs, a

# ---------------------------
# Build Ising cost Hamiltonian
# ---------------------------
def build_cost_hamiltonian(classical_costs, m_table, penalty_weight=0.01):
    feasible_indices = np.where(m_table == 0)[0]
    feasible_costs = classical_costs[feasible_indices]


    min_feasible = np.min(feasible_costs)
    max_feasible = np.max(feasible_costs)
    cost_range = max_feasible - min_feasible + 1e-6

    normalized_costs = np.zeros(len(classical_costs))
    for l in range(len(classical_costs)):
        if m_table[l] == 0:
            normalized_costs[l] = (classical_costs[l] - min_feasible) / cost_range
        else:
            normalized_costs[l] = 10.0

    print(f"Feasible cost range: [{min_feasible:.3f}, {max_feasible:.3f}]")
    print(f"Normalized costs (for H_C):\n{normalized_costs.round(3)}\n")

    coeffs = []
    ops = []
    # main cost
    for l in range(L):
        hz = normalized_costs[l]
        coeffs.append(hz)
        ops.append(qml.PauliZ(l))

    penalty_coeff = -penalty_weight
    for l in range(L):
        coeffs.append(penalty_coeff)
        ops.append(qml.PauliZ(l))
    print(f"Added penalty term with weight: {penalty_weight}")
    print(f"Total Hamiltonian terms: {len(coeffs)}\n")
    return qml.Hamiltonian(coeffs, ops)

# ---------------------------
# XY ring mixer
# ---------------------------
def build_xy_ring_mixer(L):
    coeffs = []
    ops = []
    for l in range(L):
        j = (l + 1) % L
        coeffs.append(1.0)
        ops.append(qml.PauliX(l) @ qml.PauliX(j))
        coeffs.append(1.0)
        ops.append(qml.PauliY(l) @ qml.PauliY(j))
    return qml.Hamiltonian(coeffs, ops)

# ---------------------------
# Initialization
# ---------------------------
def prepare_one_hot_basis(index=3):
    qml.PauliX(index)

# ---------------------------
# QAOA layer
# ---------------------------
def create_qaoa_circuits(cost_h, mixer_h):
    """Create QAOA circuits for given Hamiltonians"""
    def qaoa_layer(gamma, beta):
        qaoa.cost_layer(gamma, cost_h)
        qaoa.mixer_layer(beta, mixer_h)

    @qml.qnode(dev)
    def cost_expectation(params):
        gammas, betas = params
        prepare_one_hot_basis(index=3)
        qml.layer(qaoa_layer, depth_p, gammas, betas)
        return qml.expval(cost_h)

    @qml.qnode(dev)
    def probs_circuit(params):
        gammas, betas = params
        prepare_one_hot_basis(index=3)
        qml.layer(qaoa_layer, depth_p, gammas, betas)
        return qml.probs(wires=range(L))

    return cost_expectation, probs_circuit

# ---------------------------
# Learning rate schedule
# ---------------------------
def get_stepsize(iteration, total_steps, stepsize_init, stepsize_final):

    return stepsize_final + 0.5 * (stepsize_init - stepsize_final) * (1 + np.cos(np.pi * iteration / total_steps))

# ---------------------------
# Optimize for a single user
# ---------------------------
def optimize_single_user(user_id, N_users, c_total_table, m_table, cost_expectation, probs_circuit, user_seed=None):
    """Run QAOA optimization for a single user"""
    if user_seed is None:
        user_seed = seed + user_id
    rng = np.random.default_rng(user_seed)

    init_gammas = 0.1 * rng.standard_normal(size=(depth_p,))
    init_betas = 0.1 * rng.standard_normal(size=(depth_p,))
    params = qml.numpy.array([init_gammas, init_betas], requires_grad=True)

    opt = qml.AdamOptimizer(stepsize_init)

    cost_history = []
    selected_latency_history = []
    expected_latency_history = []

    # Track best solution found so far
    best_latency_so_far = np.inf
    best_latency_history = []


    initial_idx = 3
    initial_latency = c_total_table[initial_idx]
    initial_power = power_levels[initial_idx]

    if N_users == 1:
        print("--- Starting Optimization ---")
        print(f"Initial State: Power={initial_power:.4f}W (index {initial_idx}), c_total={initial_latency:.3f}s")
        print(f"Learning rate schedule: {stepsize_init:.4f} â†’ {stepsize_final:.4f}")
    else:
        if user_id % 10 == 0:
            print(f"--- Starting Optimization for User {user_id+1}/{N_users} ---")
            print(f"Initial State: Power={initial_power:.4f}W (index {initial_idx}), c_total={initial_latency:.3f}s")

    for it in range(steps):

        current_stepsize = get_stepsize(it, steps, stepsize_init, stepsize_final)
        opt.stepsize = current_stepsize

        params, prev_cost = opt.step_and_cost(cost_expectation, params)
        cost_history.append(prev_cost)

        probs = probs_circuit(params)

        one_hot_indices = [1 << l for l in range(L)]
        one_hot_probs = probs[one_hot_indices]

        prob_sum = np.sum(one_hot_probs) + 1e-9
        one_hot_probs_normalized = one_hot_probs / prob_sum

        best_idx_iter = int(np.argmax(one_hot_probs_normalized))
        latency_iter = c_total_table[best_idx_iter]
        selected_latency_history.append(latency_iter)

        expected_latency_iter = np.sum(one_hot_probs_normalized * c_total_table)  # Use TOTAL latency
        expected_latency_history.append(expected_latency_iter)


        if expected_latency_iter < best_latency_so_far:
            best_latency_so_far = expected_latency_iter
        best_latency_history.append(best_latency_so_far)

    # Final results for this user
    probs = probs_circuit(params)
    one_hot_indices = [1 << l for l in range(L)]
    one_hot_probs = probs[one_hot_indices]
    prob_sum = np.sum(one_hot_probs) + 1e-9
    one_hot_probs_normalized = one_hot_probs / prob_sum
    best_idx = int(np.argmax(one_hot_probs_normalized))
    best_power = power_levels[best_idx]
    best_latency = c_total_table[best_idx]
    mask_val = m_table[best_idx]
    return {
        'user_id': user_id,
        'params': params,
        'cost_history': cost_history,
        'selected_latency_history': selected_latency_history,
        'expected_latency_history': expected_latency_history,
        'best_latency_history': best_latency_history,
        'best_idx': best_idx,
        'best_power': best_power,
        'best_latency': best_latency,
        'mask_val': mask_val,
        'final_probs': one_hot_probs_normalized
    }

# ---------------------------
# Main loop: Run for N=100, 500, 1000
# ---------------------------
all_results = {}

for N_users in N_users_list:
    print("\n" + "="*80)
    print(f"RUNNING FOR N_users = {N_users}")
    print("="*80 + "\n")


    c_total_table, m_table, classical_costs, a = build_problem_for_N_users(N_users)


    cost_h = build_cost_hamiltonian(classical_costs, m_table, penalty_weight=0.01)
    mixer_h = build_xy_ring_mixer(L)

    # Create circuits
    cost_expectation, probs_circuit = create_qaoa_circuits(cost_h, mixer_h)

    # Run optimization for all users
    print("\n" + "="*60)
    print(f"Running QAOA optimization for {N_users} users independently...")
    print("="*60 + "\n")

    user_results = []
    for u in range(N_users):
        result = optimize_single_user(u, N_users, c_total_table, m_table,
                                     cost_expectation, probs_circuit)
        user_results.append(result)
        if u % 50 == 0 or u == N_users - 1:
            print(f"Completed {u+1}/{N_users} users...")

    cumulative_best_latency_history = np.sum([r['best_latency_history'] for r in user_results], axis=0)

    # Store results
    all_results[N_users] = {
        'user_results': user_results,
        'cumulative_best_latency_history': cumulative_best_latency_history,
        'final_latency': cumulative_best_latency_history[-1]
    }



# ---------------------------
# Plot results
# ---------------------------
print("\n--- Generating Final Plot ---")
fig, axes = plt.subplots(1, 3, figsize=(15, 4))

colors = ['blue', 'orangered', 'gold']
for idx, N_users in enumerate(N_users_list):
    ax = axes[idx]
    data = all_results[N_users]
    latency_history = data['cumulative_best_latency_history']
    final_value = data['final_latency']

    # Plot convergence
    ax.plot(range(1, len(latency_history) + 1), latency_history,
            color=colors[idx], linewidth=2, marker='o', markersize=3)
    ax.axhline(y=final_value, color='gray', linestyle=':', linewidth=1.5, alpha=0.7)
    ax.set_xlabel('Iteration', fontsize=11)
    ax.set_ylabel('Latency $T_{tot}$ (s)', fontsize=11)
    ax.set_title(f'N = {N_users}', fontsize=13, fontweight='bold')
    ax.grid(True, linestyle='--', alpha=0.4)

plt.tight_layout()
plt.savefig('qaoa_latency_convergence.png', dpi=150, bbox_inches='tight')
print("Plot saved as 'qaoa_latency_convergence.png'")
plt.show()

print("\nDone.")
